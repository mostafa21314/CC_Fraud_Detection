{
  "num_hidden_layers": 2,
  "hidden_size": 128,
  "activation": "leaky_relu",
  "dropout": 0.2,
  "weight_decay": 0.0001,
  "learning_rate": 0.01,
  "batch_size": 2048,
  "num_epochs": 8,
  "loss_type": "mixed",
  "pos_weight": 5.0,
  "alpha": 0.8,
  "gamma": 1.0,
  "lam": 0.7
}